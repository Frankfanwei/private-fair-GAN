{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAPF for the UCI Adult Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vZlT-sXHCj6D",
    "outputId": "e33998cc-0125-4fde-9896-a17d4bb1b338"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SLmaTAYCnVl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country  class  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/adult.data\", sep=\", \")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['class', 'fnlwgt', 'capital-loss', 'capital-gain'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders = {}\n",
    "X_num = X.copy()\n",
    "\n",
    "for col in X_num.columns.tolist():\n",
    "    if X_num[col].dtype == object:\n",
    "        encoders[col] = preprocessing.LabelEncoder().fit(X_num[col])\n",
    "        X_num[col] = encoders[col].transform(X_num[col])\n",
    "X_num = X_num.drop(['race'], axis=1)\n",
    "\n",
    "y = data['class'].copy()\n",
    "y = y.replace(\"<=50K\", 0)\n",
    "y = y.replace(\">50K\", 1)\n",
    "\n",
    "s = encoders['race'].transform(X['race'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0MGvrk4gDQdm"
   },
   "source": [
    "## Create Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([32561, 10]) X_noised torch.Size([32561, 15]) s (32561,) y (32561,)\n"
     ]
    }
   ],
   "source": [
    "X_tensor = torch.tensor(X_num.values, device=device).double()\n",
    "noise = torch.randn([X_tensor.shape[0], 5], device=device).double()\n",
    "X_noised = torch.cat((X_tensor, noise), 1)\n",
    "\n",
    "s_tensor = torch.tensor(s, device=device).double().unsqueeze(1)\n",
    "y_tensor = torch.tensor(y.values, device=device).double().squeeze()\n",
    "print(\"X\", X_tensor.shape, \"X_noised\", X_noised.shape, \"s\", s.shape, \"y\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H0KYrZGdD7OD"
   },
   "source": [
    "## Create Models\n",
    "\n",
    "Since we only have 11 features, only append a size 5 vector of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8X4g8jkhDkw_"
   },
   "outputs": [],
   "source": [
    "model_gen = torch.nn.Sequential(\n",
    "          # Layer 1 - 15 -> 128\n",
    "          torch.nn.Linear(15, 128),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(128),\n",
    "          # Layer 2 - 128 -> 254\n",
    "          torch.nn.Linear(128, 256),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(256),\n",
    "          # Layer 3 - 256 -> 256\n",
    "          torch.nn.Linear(256, 256),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(256),\n",
    "          # Output\n",
    "          torch.nn.Linear(256, 10),\n",
    "        ).to(device)\n",
    "\n",
    "model_adv = torch.nn.Sequential(\n",
    "          # Layer 1 - 10 -> 128\n",
    "          torch.nn.Linear(10, 128),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(128),\n",
    "          # Layer 2 - 128 -> 256\n",
    "          torch.nn.Linear(128, 256),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(256),\n",
    "          # Layer 3 - 256 -> 256\n",
    "          torch.nn.Linear(256, 256),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(256),\n",
    "          # Layer 4 - 256 -> 128\n",
    "          torch.nn.Linear(256, 128),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(128),\n",
    "          # Output - 128 -> 5\n",
    "          torch.nn.Linear(128, 5),\n",
    "        ).to(device)\n",
    "\n",
    "model_class = torch.nn.Sequential(\n",
    "          # Layer 1 - 10 -> 128\n",
    "          torch.nn.Linear(10, 128),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(128),\n",
    "          # Layer 2 - 128 -> 256\n",
    "          torch.nn.Linear(256, 256),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(256),\n",
    "          # Layer 3 - 256 -> 256\n",
    "          torch.nn.Linear(256, 256),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(256),\n",
    "          # Layer 4 - 256 -> 128\n",
    "          torch.nn.Linear(256, 128),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(128),\n",
    "          # Output - 128 -> 2\n",
    "          torch.nn.Linear(128, 2),\n",
    "        ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UoBSQqTDEPTj"
   },
   "outputs": [],
   "source": [
    "optim_gen = torch.optim.Adam(model_gen.parameters())\n",
    "loss_gen = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optim_adv = torch.optim.Adam(model_adv.parameters())\n",
    "loss_adv = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "optim_class = torch.optim.Adam(model_class.parameters())\n",
    "loss_class = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tELOv3FsFt63"
   },
   "source": [
    "## Train Adversarially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1129
    },
    "colab_type": "code",
    "id": "TFepEWbJaWeb",
    "outputId": "7b251e4c-5f81-4025-fa49-9b5b8ed4087b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Gen loss:  155.5788116455078\n",
      "Gen loss:  138.99288940429688\n",
      "Gen loss:  103.40608215332031\n",
      "Gen loss:  48.600929260253906\n",
      "Gen loss:  5.056375503540039\n",
      "Adv loss:  1.2881985902786255\n",
      "\n",
      "\n",
      "Epoch:  1\n",
      "Gen loss:  3.69512677192688\n",
      "Gen loss:  3.350835084915161\n",
      "Gen loss:  3.4352047443389893\n",
      "Gen loss:  2.7001357078552246\n",
      "Gen loss:  2.9146783351898193\n",
      "Adv loss:  0.5999350547790527\n",
      "\n",
      "\n",
      "Epoch:  2\n",
      "Gen loss:  2.84016489982605\n",
      "Gen loss:  2.9173569679260254\n",
      "Gen loss:  3.0586581230163574\n",
      "Gen loss:  2.828970432281494\n",
      "Gen loss:  2.8840126991271973\n",
      "Adv loss:  0.4987228512763977\n",
      "\n",
      "\n",
      "Epoch:  3\n",
      "Gen loss:  2.559046983718872\n",
      "Gen loss:  2.0644705295562744\n",
      "Gen loss:  2.6670851707458496\n",
      "Gen loss:  2.750171422958374\n",
      "Gen loss:  3.052269220352173\n",
      "Adv loss:  0.48674190044403076\n",
      "\n",
      "\n",
      "Epoch:  4\n",
      "Gen loss:  2.3310019969940186\n",
      "Gen loss:  2.5449717044830322\n",
      "Gen loss:  2.4900150299072266\n",
      "Gen loss:  2.662214517593384\n",
      "Gen loss:  1.9178998470306396\n",
      "Adv loss:  0.47968077659606934\n",
      "\n",
      "\n",
      "Epoch:  5\n",
      "Gen loss:  2.5488498210906982\n",
      "Gen loss:  1.9928960800170898\n",
      "Gen loss:  2.8147590160369873\n",
      "Gen loss:  2.670851707458496\n",
      "Gen loss:  2.5557236671447754\n",
      "Adv loss:  0.47588738799095154\n",
      "\n",
      "\n",
      "Epoch:  6\n",
      "Gen loss:  2.503324270248413\n",
      "Gen loss:  1.7051115036010742\n",
      "Gen loss:  2.2445292472839355\n",
      "Gen loss:  2.240021228790283\n",
      "Gen loss:  2.6950953006744385\n",
      "Adv loss:  0.4753040671348572\n",
      "\n",
      "\n",
      "Epoch:  7\n",
      "Gen loss:  2.353672504425049\n",
      "Gen loss:  1.8998925685882568\n",
      "Gen loss:  2.0307703018188477\n",
      "Gen loss:  2.599635124206543\n",
      "Gen loss:  2.345513105392456\n",
      "Adv loss:  0.4693906605243683\n",
      "\n",
      "\n",
      "Epoch:  8\n",
      "Gen loss:  2.4952988624572754\n",
      "Gen loss:  2.8314027786254883\n",
      "Gen loss:  1.7350077629089355\n",
      "Gen loss:  2.1927108764648438\n",
      "Gen loss:  2.6800529956817627\n",
      "Adv loss:  0.46962666511535645\n",
      "\n",
      "\n",
      "Epoch:  9\n",
      "Gen loss:  1.8869342803955078\n",
      "Gen loss:  2.3716063499450684\n",
      "Gen loss:  2.334254741668701\n",
      "Gen loss:  1.6161023378372192\n",
      "Gen loss:  2.2262401580810547\n",
      "Adv loss:  0.4677324593067169\n",
      "\n",
      "\n",
      "Epoch:  10\n",
      "Gen loss:  2.01818585395813\n",
      "Gen loss:  2.0035881996154785\n",
      "Gen loss:  2.397848129272461\n",
      "Gen loss:  1.8595616817474365\n",
      "Gen loss:  2.1666812896728516\n",
      "Adv loss:  0.46845006942749023\n",
      "\n",
      "\n",
      "Epoch:  11\n",
      "Gen loss:  2.5107126235961914\n",
      "Gen loss:  2.0243847370147705\n",
      "Gen loss:  1.9980201721191406\n",
      "Gen loss:  1.6626274585723877\n",
      "Gen loss:  1.9314948320388794\n",
      "Adv loss:  0.46277114748954773\n",
      "\n",
      "\n",
      "Epoch:  12\n",
      "Gen loss:  2.0077731609344482\n",
      "Gen loss:  1.6901084184646606\n",
      "Gen loss:  2.1026625633239746\n",
      "Gen loss:  2.01706862449646\n",
      "Gen loss:  2.002864360809326\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS_GEN = 5\n",
    "NUM_EPOCHS_ADV = 1\n",
    "NUM_TOTAL_ITER = 15\n",
    "DISTORTION_WEIGHT = 0.1\n",
    "D = 3\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.cat((X_noised, s_tensor), 1), \n",
    "    batch_size=512, \n",
    "    shuffle=True)\n",
    "\n",
    "loss_by_epoch_g = []\n",
    "loss_by_epoch_a = []\n",
    "\n",
    "for epoch in range(NUM_TOTAL_ITER):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    \n",
    "    for j in range(NUM_EPOCHS_GEN):\n",
    "        total_loss_g = 0\n",
    "        total_loss_d = 0\n",
    "        num = 0\n",
    "        for batch in train_loader:\n",
    "            x, s = batch[:, 0:-1], batch[:, -1].long()\n",
    "            x_hat = model_gen(x.float())\n",
    "            adv_pred = model_adv(x_hat.float())\n",
    "\n",
    "            loss_g = -loss_adv(adv_pred, s)\n",
    "            dist_loss = torch.dist(x_hat, x[:, 0:10].float()) * DISTORTION_WEIGHT\n",
    "            if dist_loss < D:\n",
    "                dist_loss = 0\n",
    "\n",
    "            total_loss_d += dist_loss\n",
    "            loss_g += dist_loss\n",
    "\n",
    "            num += 1\n",
    "            total_loss_g += loss_g\n",
    "\n",
    "            optim_gen.zero_grad()\n",
    "            loss_g.backward()\n",
    "            optim_gen.step()\n",
    "        epch_loss = (total_loss_g/num).item()\n",
    "        loss_by_epoch_g.append(epch_loss)\n",
    "        print(\"Gen loss: \", epch_loss)\n",
    "\n",
    "    for j in range(NUM_EPOCHS_ADV):\n",
    "        total_loss_a = 0\n",
    "        num = 0\n",
    "        for batch in train_loader:\n",
    "            x, s = batch[:, 0:-1], batch[:, -1].long()\n",
    "\n",
    "            x_hat = model_gen(x.float())\n",
    "\n",
    "            s_pred = model_adv(x_hat)\n",
    "\n",
    "            loss_a = loss_adv(s_pred, s)\n",
    "            num += 1\n",
    "            total_loss_a += loss_a\n",
    "\n",
    "            optim_adv.zero_grad()\n",
    "            loss_a.backward(retain_graph=True)\n",
    "            optim_adv.step()\n",
    "        epch_loss = (total_loss_a/num).item()\n",
    "        loss_by_epoch_a.append(epch_loss)\n",
    "        print(\"Adv loss: \", (total_loss_a/num).item())\n",
    "        print(\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "u7cKU9akwXbZ",
    "outputId": "e1afac70-7c20-46fe-c780-84e623272e03"
   },
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "ax.set(xlabel=\"Epochs\", ylabel=\"Generator Loss\", Title=\"Generator Loss Curve W/ D=7\")\n",
    "ax.plot(range(NUM_TOTAL_ITER * NUM_EPOCHS_GEN), loss_by_epoch_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "FjHnjopBwzDq",
    "outputId": "af8e3423-1e7f-490c-b4c1-2f648e1e0678"
   },
   "outputs": [],
   "source": [
    "ax = plt.axes()\n",
    "ax.set(xlabel=\"Epochs\", ylabel=\"Generator Loss\", Title=\"Generator Loss Curve W/ D=7\")\n",
    "ax.plot(range(NUM_TOTAL_ITER * NUM_EPOCHS_ADV), loss_by_epoch_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGuCZ_g2NJeB"
   },
   "source": [
    "# Testing\n",
    "## Test Adversary before and after decorrelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mvY9HkCNOUGP",
    "outputId": "4255bdba-9ab8-49ad-a889-bcff6f8ec594"
   },
   "outputs": [],
   "source": [
    "print(X_tensor.shape)\n",
    "out_class = model_adv(X_tensor.float())\n",
    "v, i = torch.max(out_class, 1)\n",
    "print((s_tensor.squeeze().int() == i.int()).nonzero().shape[0]/s_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vv1BVNCJNM2h",
    "outputId": "6f34084f-e440-4813-e493-d86b77199461"
   },
   "outputs": [],
   "source": [
    "gen_noised = model_gen(noised_tensor.float())\n",
    "\n",
    "out_class = model_adv(gen_noised.float())\n",
    "v, i = torch.max(out_class, 1)\n",
    "print((s_tensor.squeeze().int() == i.int()).nonzero().shape[0]/s_tensor.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xU2EPwMrNNc7"
   },
   "source": [
    "## Test Classifier Before and After Decorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXp6RnXLPUQa"
   },
   "source": [
    "### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "tQgO2UqtNPfQ",
    "outputId": "b6ff98a8-7720-4078-fe34-39d081383fe6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.6459, device='cuda:0')\n",
      "loss:  tensor(0.1994, device='cuda:0')\n",
      "loss:  tensor(0.1168, device='cuda:0')\n",
      "loss:  tensor(0.0851, device='cuda:0')\n",
      "loss:  tensor(0.0632, device='cuda:0')\n",
      "loss:  tensor(0.0522, device='cuda:0')\n",
      "loss:  tensor(0.0491, device='cuda:0')\n",
      "loss:  tensor(0.0460, device='cuda:0')\n",
      "loss:  tensor(0.0402, device='cuda:0')\n",
      "loss:  tensor(0.0358, device='cuda:0')\n",
      "loss:  tensor(0.0389, device='cuda:0')\n",
      "loss:  tensor(0.0344, device='cuda:0')\n",
      "loss:  tensor(0.0339, device='cuda:0')\n",
      "loss:  tensor(0.0299, device='cuda:0')\n",
      "loss:  tensor(0.0296, device='cuda:0')\n",
      "loss:  tensor(0.0241, device='cuda:0')\n",
      "loss:  tensor(0.0247, device='cuda:0')\n",
      "loss:  tensor(0.0245, device='cuda:0')\n",
      "loss:  tensor(0.0221, device='cuda:0')\n",
      "loss:  tensor(0.0152, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class_loader = torch.utils.data.DataLoader(\n",
    "    torch.cat((X_tensor, y_tensor), 1), \n",
    "    batch_size=512, \n",
    "    shuffle=True)\n",
    "\n",
    "for epoch in range(20):\n",
    "  loss_avg = 0\n",
    "  num = 0\n",
    "  for batch in class_loader:\n",
    "    x, y = batch[:, 0:-1], batch[:, -1]\n",
    "    y_pred = model_class(x.float())\n",
    "\n",
    "    loss = loss_class(y_pred, y.long())\n",
    "    loss_avg += loss\n",
    "    num += 1\n",
    "\n",
    "    optim_class.zero_grad()\n",
    "    loss.backward()\n",
    "    optim_class.step()\n",
    "  print(\"loss: \", (loss_avg/num).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WPK7X3LHOALe",
    "outputId": "ab426802-8d5c-4d90-a28e-4ca1054cd7fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9589412962334578\n"
     ]
    }
   ],
   "source": [
    "out_class = model_class(X_tensor_test.float())\n",
    "v, i = torch.max(out_class, 1)\n",
    "print((y_tensor_test.squeeze().int() == i.int()).nonzero().shape[0]/y_tensor_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVJmzScUPWRV"
   },
   "source": [
    "### After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "9pN8lxieOnt0",
    "outputId": "9360182c-8ef4-433c-9c52-2501e886b19b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.6359238624572754\n",
      "loss:  0.22694246470928192\n",
      "loss:  0.13943733274936676\n",
      "loss:  0.1030813604593277\n",
      "loss:  0.08952376246452332\n",
      "loss:  0.08020778000354767\n",
      "loss:  0.06571952998638153\n",
      "loss:  0.06776061654090881\n",
      "loss:  0.05831295996904373\n",
      "loss:  0.0489499606192112\n",
      "loss:  0.047430653125047684\n",
      "loss:  0.04686059057712555\n",
      "loss:  0.04153882712125778\n",
      "loss:  0.03858698159456253\n",
      "loss:  0.04043963924050331\n",
      "loss:  0.05129847303032875\n",
      "loss:  0.053298063576221466\n",
      "loss:  0.03892166167497635\n",
      "loss:  0.03216115012764931\n",
      "loss:  0.027386115863919258\n"
     ]
    }
   ],
   "source": [
    "model_class = torch.nn.Sequential(\n",
    "          # Layer 1 - 561 -> 512\n",
    "          torch.nn.Linear(561, 512),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(512),\n",
    "          # Layer 2 - 512 -> 512\n",
    "          torch.nn.Linear(512, 512),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(512),\n",
    "          # Layer 3 - 512 -> 256\n",
    "          torch.nn.Linear(512, 256),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(256),\n",
    "          # Layer 4 - 256 -> 128\n",
    "          torch.nn.Linear(256, 128),\n",
    "          torch.nn.LeakyReLU(),\n",
    "          torch.nn.BatchNorm1d(128),\n",
    "          # Output - 128 -> 6\n",
    "          torch.nn.Linear(128, 6),\n",
    "        ).to(device)\n",
    "optim_class = torch.optim.Adam(model_class.parameters())\n",
    "loss_class = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "gen_noised = model_gen(noised_tensor.float())\n",
    "\n",
    "class_loader = torch.utils.data.DataLoader(\n",
    "    torch.cat((gen_noised.float(), y_tensor.float()), 1), \n",
    "    batch_size=512, \n",
    "    shuffle=True)\n",
    "\n",
    "for epoch in range(20):\n",
    "  loss_avg = 0\n",
    "  num = 0\n",
    "  for batch in class_loader:\n",
    "    x, y = batch[:, 0:-1], batch[:, -1]\n",
    "    y_pred = model_class(x.float())\n",
    "\n",
    "    loss = loss_class(y_pred, y.long())\n",
    "    loss_avg += loss\n",
    "    num += 1\n",
    "\n",
    "    optim_class.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optim_class.step()\n",
    "  print(\"loss: \", (loss_avg/num).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d3jsTYFcPkCl",
    "outputId": "e1e6b410-3444-4e32-b96c-e1c9ef60b19f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9477434679334917\n"
     ]
    }
   ],
   "source": [
    "out_class = model_class(X_tensor_test.float())\n",
    "v, i = torch.max(out_class, 1)\n",
    "print((y_tensor_test.squeeze().int() == i.int()).nonzero().shape[0]/y_tensor_test.shape[0])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ICLR Reproducibility Challenge Private and Fair Representations (GAN)",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
